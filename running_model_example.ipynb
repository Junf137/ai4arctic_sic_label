{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando/anaconda3/envs/AI4Artic2/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from icecream import ic\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmcv import Config, mkdir_or_exist\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "import wandb\n",
    "# Functions to calculate metrics and show the relevant chart colorbar.\n",
    "from functions import compute_metrics, save_best_model, load_model, slide_inference, batched_slide_inference\n",
    "# Custom dataloaders for regular training and validation.\n",
    "from loaders import (AI4ArcticChallengeDataset, AI4ArcticChallengeTestDataset,\n",
    "                     get_variable_options)\n",
    "#  get_variable_options\n",
    "from unet import UNet  # Convolutional Neural Network model\n",
    "from swin_transformer import SwinTransformer  # Swin Transformer\n",
    "# -- Built-in modules -- #\n",
    "from utils import colour_str\n",
    "\n",
    "from test_upload_function import test\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def create_train_and_validation_scene_list(train_options):\n",
    "    '''\n",
    "    Creates the a train and validation scene list. Adds these two list to the config file train_options\n",
    "\n",
    "    '''\n",
    "    with open(train_options['path_to_env'] + train_options['train_list_path']) as file:\n",
    "        train_options['train_list'] = json.loads(file.read())\n",
    "\n",
    "    # Convert the original scene names to the preprocessed names.\n",
    "    train_options['train_list'] = [file[17:32] + '_' + file[77:80] +\n",
    "                                   '_prep.nc' for file in train_options['train_list']]\n",
    "\n",
    "    # # Select a random number of validation scenes with the same seed. Feel free to change the seed.et\n",
    "    # # np.random.seed(0)\n",
    "    # train_options['validate_list'] = np.random.choice(np.array(\n",
    "    #     train_options['train_list']), size=train_options['num_val_scenes'], replace=False)\n",
    "\n",
    "    # load validation list\n",
    "    with open(train_options['path_to_env'] + train_options['val_path']) as file:\n",
    "        train_options['validate_list'] = json.loads(file.read())\n",
    "    # Convert the original scene names to the preprocessed names.\n",
    "    train_options['validate_list'] = [file[17:32] + '_' + file[77:80] +\n",
    "                                      '_prep.nc' for file in train_options['validate_list']]\n",
    "\n",
    "    # from icecream import ic\n",
    "    # ic(train_options['validate_list'])\n",
    "    # Remove the validation scenes from the train list.\n",
    "    train_options['train_list'] = [scene for scene in train_options['train_list']\n",
    "                                   if scene not in train_options['validate_list']]\n",
    "    print('Options initialised')\n",
    "\n",
    "\n",
    "def create_dataloaders(train_options):\n",
    "    '''\n",
    "    Create train and validation dataloader based on the train and validation list inside train_options.\n",
    "\n",
    "    '''\n",
    "    # Custom dataset and dataloader.\n",
    "    dataset = AI4ArcticChallengeDataset(\n",
    "        files=train_options['train_list'], options=train_options, do_transform=True)\n",
    "\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=None, shuffle=True, num_workers=train_options['num_workers'], pin_memory=True)\n",
    "    # - Setup of the validation dataset/dataloader. The same is used for model testing in 'test_upload.ipynb'.\n",
    "\n",
    "    dataset_val = AI4ArcticChallengeTestDataset(\n",
    "        options=train_options, files=train_options['validate_list'], mode='train_val')\n",
    "\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=None, num_workers=train_options['num_workers_val'], shuffle=False)\n",
    "\n",
    "    return dataloader_train, dataloader_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| config_path: 'configs/edge_consitency/get_output.py'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mGPU available!\u001b[0m\n",
      "Total number of available devices:  \u001b[0;33m1\u001b[0m\n",
      "\u001b[91m Finetune model from checkpoints/best_model_ds_sgd_re_domain_adp.pth\u001b[0m\n",
      "Options initialised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:14<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_path = 'configs/edge_consitency/get_output.py'\n",
    "\n",
    "checkpoint_path = 'checkpoints/best_model_ds_sgd_re_domain_adp.pth'\n",
    "\n",
    "ic(config_path)\n",
    "cfg = Config.fromfile(config_path)\n",
    "train_options = cfg.train_options\n",
    "# Get options for variables, amsrenv grid, cropping and upsampling.\n",
    "train_options = get_variable_options(train_options)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(colour_str('GPU available!', 'green'))\n",
    "    print('Total number of available devices: ',\n",
    "            colour_str(torch.cuda.device_count(), 'orange'))\n",
    "    device = torch.device(f\"cuda:{train_options['gpu_id']}\")\n",
    "else:\n",
    "    print(colour_str('GPU not available.', 'red'))\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU setup completed!')\n",
    "\n",
    "\n",
    "if train_options['model_selection'] == 'unet':\n",
    "    net = UNet(options=train_options).to(device)\n",
    "elif train_options['model_selection'] == 'swin':\n",
    "    net = SwinTransformer(options=train_options).to(device)\n",
    "elif train_options['model_selection'] == 'h_unet':\n",
    "    from unet import H_UNet\n",
    "    net = H_UNet(options=train_options).to(device)\n",
    "elif train_options['model_selection'] == 'h_unet_argmax':\n",
    "    from unet import H_UNet_argmax\n",
    "    net = H_UNet_argmax(options=train_options).to(device)    \n",
    "else:\n",
    "    raise 'Unknown model selected'\n",
    "\n",
    "print(f\"\\033[91m Finetune model from {checkpoint_path}\\033[0m\")\n",
    "_ = load_model(net, checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "create_train_and_validation_scene_list(train_options)\n",
    "\n",
    "dataloader_train, dataloader_val = create_dataloaders(train_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter =iter(dataloader_val)\n",
    "net.eval() \n",
    "\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    batch = next(val_iter)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        output = net(batch[0].to(device, non_blocking=True))\n",
    "        for chart in train_options['charts']:\n",
    "            output[chart].to('cpu').detach()\n",
    "    outputs.append(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pickle\n",
    "with open('model_outputs', 'wb') as model_outputs_file:\n",
    " \n",
    "  # Step 3\n",
    "  pickle.dump(outputs, model_outputs_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI4Artic2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
